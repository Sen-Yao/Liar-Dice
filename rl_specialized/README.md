# RL Specialized Models - ä¸“ç”¨æ¨¡å‹å¼ºåŒ–å­¦ä¹ 

## ğŸ“– æ¦‚è¿°

è¿™ä¸ªæ¨¡å—å®ç°äº†é’ˆå¯¹ä¸åŒç©å®¶æ•°é‡ï¼ˆ2-6äººï¼‰çš„ä¸“ç”¨å¼ºåŒ–å­¦ä¹ æ¨¡å‹ã€‚ä¸é€šç”¨æ¨¡å‹ä¸åŒï¼Œä¸“ç”¨æ¨¡å‹ä¸ºæ¯ç§ç©å®¶é…ç½®è®­ç»ƒç‹¬ç«‹çš„ç¥ç»ç½‘ç»œï¼Œä»¥è·å¾—æœ€ä¼˜çš„æ€§èƒ½è¡¨ç°ã€‚

## ğŸ—ï¸ æ¶æ„è®¾è®¡

### æ ¸å¿ƒæ€æƒ³
- **ä¸“ç”¨ä¼˜åŒ–**ï¼šæ¯ä¸ªç©å®¶æ•°é‡ä½¿ç”¨ç‹¬ç«‹çš„åŠ¨ä½œç©ºé—´å’Œæ¨¡å‹
- **é«˜æ•ˆè®­ç»ƒ**ï¼šåŠ¨ä½œç©ºé—´ç²¾ç¡®åŒ¹é…æ¸¸æˆé…ç½®ï¼Œå‡å°‘æ— æ•ˆåŠ¨ä½œ
- **æ€§èƒ½æœ€ä¼˜**ï¼šé’ˆå¯¹ç‰¹å®šåœºæ™¯æ·±åº¦ä¼˜åŒ–ï¼Œé¿å…é€šç”¨æ¨¡å‹çš„å¤æ‚æ€§

### ç›®å½•ç»“æ„

```
rl_specialized/
â”œâ”€â”€ action_spaces/          # åŠ¨ä½œç©ºé—´å®šä¹‰
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ base.py            # åŠ¨ä½œç©ºé—´åŸºç¡€æŠ½è±¡ç±»
â”‚   â””â”€â”€ player_specific.py # ä¸“ç”¨æ¨¡å‹åŠ¨ä½œç©ºé—´å®ç°
â”œâ”€â”€ agents/                # RLæ™ºèƒ½ä½“å®ç°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ specialized_agent.py (å¾…å®ç°)
â”œâ”€â”€ networks/              # ç¥ç»ç½‘ç»œç»“æ„
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ policy_network.py (å¾…å®ç°)
â”œâ”€â”€ training/              # è®­ç»ƒè„šæœ¬
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ train_specialized.py (å¾…å®ç°)
â”œâ”€â”€ utils/                 # å·¥å…·å‡½æ•°
â”‚   â”œâ”€â”€ __init__.py
â”‚   â””â”€â”€ state_encoder.py  # çŠ¶æ€ç¼–ç å™¨
â””â”€â”€ README.md             # æœ¬æ–‡ä»¶
```

## ğŸ¯ åŠ¨ä½œç©ºé—´è®¾è®¡

### åŠ¨ä½œæ˜ å°„è§„åˆ™ï¼ˆä¸“ç”¨ï¼‰

ä¸“ç”¨æ¨¡å‹å¯¹åŠ¨ä½œç©ºé—´è¿›è¡Œâ€œé¦–è½®è§„åˆ™åŒ–â€è£å‰ªï¼šè®¡æ•°ä» n+1 å¼€å§‹ï¼ˆn=ç©å®¶æ•°ï¼‰ã€‚

```python
# ä»¥2äººæ¸¸æˆä¸ºä¾‹
num_players = 2
dice_per_player = 5
min_count = n + 1 = 3
max_count = n * 5 = 10
counts_per_mode = max_count - min_count + 1 = 8

total_actions = 1 (challenge) + 2 * counts_per_mode * 6 = 97

# åŠ¨ä½œIDåˆ†é…ï¼š
- Challenge: action_id = 0
- æ–‹æ¨¡å¼çŒœæµ‹: action_id = 1 + (count-min_count)*6 + (face-1)
- é£æ¨¡å¼çŒœæµ‹: action_id = 1 + counts_per_mode*6 + (count-min_count)*6 + (face-1)
```

### ä¸åŒç©å®¶æ•°é‡çš„åŠ¨ä½œç©ºé—´å¤§å°

| ç©å®¶æ•°é‡ | æœ€å¤§éª°å­æ•° | è®¡æ•°èŒƒå›´ï¼ˆå«ï¼‰ | åŠ¨ä½œç©ºé—´å¤§å° |
|---------|-----------|---------------|-------------|
| 2äºº     | 10        | 3..10         | 97          |
| 3äºº     | 15        | 4..15         | 145         |
| 4äºº     | 20        | 5..20         | 193         |
| 5äºº     | 25        | 6..25         | 241         |
| 6äºº     | 30        | 7..30         | 289         |

## ğŸ§  æ ¸å¿ƒç»„ä»¶

### 1. BaseActionSpace (base.py)
- æŠ½è±¡åŸºç±»ï¼Œå®šä¹‰åŠ¨ä½œç©ºé—´çš„é€šç”¨æ¥å£
- å®ç°æ¸¸æˆè§„åˆ™éªŒè¯é€»è¾‘
- æä¾›è·¨æ¨¡å¼æ¯”è¾ƒç®—æ³•

### 2. PlayerSpecificActionSpace (player_specific.py)
- ä¸“ç”¨æ¨¡å‹çš„å…·ä½“å®ç°
- é«˜æ•ˆçš„åŠ¨ä½œIDä¸å¯¹è±¡äº’è½¬
- åˆæ³•åŠ¨ä½œæ©ç ç”Ÿæˆ
- é¢„å®šä¹‰2-6äººæ¸¸æˆé…ç½®

### 3. StateEncoder (state_encoder.py)
- å°†å¤æ‚æ¸¸æˆçŠ¶æ€ç¼–ç ä¸ºç¥ç»ç½‘ç»œè¾“å…¥
- å›ºå®šé•¿åº¦ç‰¹å¾å‘é‡ï¼šæ‰‹ç‰Œ(6) + ç©å®¶ä¿¡æ¯(n) + çŒœæµ‹(4) + æ¸¸æˆçŠ¶æ€(3)
- å…¼å®¹ç¯å¢ƒè¿”å›çš„ Guessï¼ˆdataclassï¼‰ä¸ dict çš„ last_guess
- æ”¯æŒæ‰¹é‡ç¼–ç å’Œç‰¹å¾åç§°æ˜ å°„

### 4. Policy Network (networks/policy_network.py)
- é¢å‘æ•°å€¼å‹çŠ¶æ€çš„é«˜æ•ˆ MLP + æ®‹å·®ç»“æ„
- æ¶æ„ï¼šLayerNorm â†’ Linear â†’ 2Ã—ResidualBlock(Linear+SiLU+Dropoutâ†’Linear+æ®‹å·®+LayerNorm) â†’ Linear
- é»˜è®¤è¾“å‡ºç‰¹å¾ç»´åº¦ `features_dim=128`ï¼Œä¾¿äºç­–ç•¥/ä»·å€¼å¤´å…±äº«
- æä¾› `MaskedStateFeatureExtractor` ç”¨äºä¸ SB3 é›†æˆï¼ˆä»è§‚å¯Ÿä¸­æ³¨å…¥åŠ¨ä½œæ©ç ï¼‰
- æä¾› `make_default_policy_kwargs()` è¾…åŠ©å‡½æ•°ï¼Œç”¨äºç”Ÿæˆ SB3 çš„ `policy_kwargs`

## ğŸš€ å¿«é€Ÿå¼€å§‹

### åˆ›å»º2äººæ¸¸æˆåŠ¨ä½œç©ºé—´

```python
from rl_specialized.action_spaces import get_2_player_action_space

# åˆ›å»ºä¸“ç”¨åŠ¨ä½œç©ºé—´
action_space = get_2_player_action_space()

# è·å–åŠ¨ä½œç©ºé—´ä¿¡æ¯
print(f"åŠ¨ä½œç©ºé—´å¤§å°: {action_space.get_action_space_size()}")
print(f"åˆ†å¸ƒä¿¡æ¯: {action_space.get_action_distribution_info()}")

# åŠ¨ä½œè½¬æ¢
action_id = 5
action_obj = action_space.id_to_action(action_id)
print(f"åŠ¨ä½œID {action_id} -> {action_obj}")

# è·å–åˆæ³•åŠ¨ä½œæ©ç 
observation = {...}  # æ¥è‡ªç¯å¢ƒçš„è§‚å¯Ÿ
mask = action_space.get_action_mask(observation)
valid_actions = action_space.get_valid_actions(observation)
```

### çŠ¶æ€ç¼–ç 

```python
from rl_specialized.utils import create_state_encoder

# åˆ›å»ºçŠ¶æ€ç¼–ç å™¨
encoder = create_state_encoder(num_players=2)

# ç¼–ç å•ä¸ªè§‚å¯Ÿ
observation = {...}  # æ¥è‡ªç¯å¢ƒ
encoded_state = encoder.encode_observation(observation)
print(f"ç¼–ç åçŠ¶æ€ç»´åº¦: {encoded_state.shape}")

# æ‰¹é‡ç¼–ç 
observations = [obs1, obs2, obs3]
batch_states = encoder.encode_batch(observations)
```

## ğŸ“Š æ€§èƒ½ç‰¹ç‚¹

### ä¼˜åŠ¿
- **å†…å­˜æ•ˆç‡**ï¼šåŠ¨ä½œç©ºé—´é’ˆå¯¹ç‰¹å®šç©å®¶æ•°é‡æœ€å°åŒ–
- **è®­ç»ƒé€Ÿåº¦**ï¼šå‡å°‘æ— æ•ˆåŠ¨ä½œæ¢ç´¢ï¼ŒåŠ å¿«æ”¶æ•›
- **æ¨¡å‹ç²¾åº¦**ï¼šä¸“é—¨ä¼˜åŒ–ï¼Œé¿å…é€šç”¨æ¨¡å‹çš„æ€§èƒ½å¦¥å
- **å¯è§£é‡Šæ€§**ï¼šæ¯ä¸ªæ¨¡å‹å¯¹åº”æ˜ç¡®çš„æ¸¸æˆé…ç½®

### é€‚ç”¨åœºæ™¯
- å›ºå®šç©å®¶æ•°é‡çš„æ¸¸æˆç¯å¢ƒ
- å¯¹æ€§èƒ½è¦æ±‚æé«˜çš„ç”Ÿäº§ç¯å¢ƒ
- éœ€è¦è¯¦ç»†åˆ†æç‰¹å®šé…ç½®çš„ç ”ç©¶åœºæ™¯
- èµ„æºå……è¶³ï¼Œå¯ä»¥ç»´æŠ¤å¤šä¸ªæ¨¡å‹çš„æƒ…å†µ

## ğŸ”§ å¼€å‘æŒ‡å—

### æ‰©å±•æ–°ç©å®¶æ•°é‡

å¦‚éœ€æ”¯æŒ7äººæˆ–æ›´å¤šç©å®¶ï¼š

1. åœ¨ `player_specific.py` ä¸­æ·»åŠ æ–°çš„å·¥å‚å‡½æ•°
2. æ›´æ–° `__init__.py` å¯¼å‡ºæ–°å‡½æ•°
3. éªŒè¯åŠ¨ä½œç©ºé—´å¤§å°å’Œå†…å­˜å ç”¨

### è‡ªå®šä¹‰çŠ¶æ€ç¼–ç 

StateEncoderæ”¯æŒç»§æ‰¿å’Œè‡ªå®šä¹‰ï¼š

```python
class CustomStateEncoder(StateEncoder):
    def encode_observation(self, observation):
        # æ·»åŠ è‡ªå®šä¹‰ç‰¹å¾
        base_features = super().encode_observation(observation)
        custom_features = self._extract_custom_features(observation)
        return np.concatenate([base_features, custom_features])
```

## ğŸ§ª æµ‹è¯•ä¸éªŒè¯

### åŠ¨ä½œç©ºé—´éªŒè¯

```python
# éªŒè¯åŠ¨ä½œæ˜ å°„çš„åŒå‘ä¸€è‡´æ€§
action_space = get_2_player_action_space()
for action_id in range(action_space.get_action_space_size()):
    action_obj = action_space.id_to_action(action_id)
    recovered_id = action_space.action_to_id(action_obj)
    assert action_id == recovered_id
```

### æ©ç æ­£ç¡®æ€§æµ‹è¯•

```python
# æµ‹è¯•åˆæ³•åŠ¨ä½œæ©ç 
observation = create_test_observation()
mask = action_space.get_action_mask(observation)
for action_id in range(len(mask)):
    if mask[action_id]:
        action_obj = action_space.id_to_action(action_id)
        assert is_legal_in_game_context(action_obj, observation)
```

## ğŸ“ˆ ä¸‹ä¸€æ­¥è®¡åˆ’

1. **å®ç°SpecializedAgentç±»** - åŸºäºä¸“ç”¨åŠ¨ä½œç©ºé—´çš„RLæ™ºèƒ½ä½“
2. **è®¾è®¡PolicyNetwork** - é’ˆå¯¹ä¸åŒç©å®¶æ•°é‡ä¼˜åŒ–çš„ç½‘ç»œç»“æ„
3. **å¼€å‘è®­ç»ƒè„šæœ¬** - æ”¯æŒå¹¶è¡Œè®­ç»ƒå¤šä¸ªä¸“ç”¨æ¨¡å‹
4. **æ€§èƒ½å¯¹æ¯”å®éªŒ** - ä¸é€šç”¨æ¨¡å‹çš„è¯¦ç»†æ€§èƒ½å¯¹æ¯”
5. **æ¨¡å‹èåˆç­–ç•¥** - æ¢ç´¢å¤šæ¨¡å‹é›†æˆçš„å¯èƒ½æ€§

## ğŸ¤ è´¡çŒ®æŒ‡å—

- éµå¾ªé¡¹ç›®ç¼–ç è§„èŒƒï¼šè‹±æ–‡å‘½å + ä¸­æ–‡æ³¨é‡Š
- ä¿æŒä»£ç ç®€æ´æ˜äº†ï¼Œé¿å…è¿‡åº¦è®¾è®¡
- æ·»åŠ å……åˆ†çš„å•å…ƒæµ‹è¯•å’Œæ–‡æ¡£
- æ€§èƒ½ä¼˜åŒ–æ—¶ä¿æŒä»£ç å¯è¯»æ€§

---

*æœ€åæ›´æ–°: 2024å¹´9æœˆ22æ—¥*
*ç‰ˆæœ¬: v1.0.0*
*ä½œè€…: éª°å­éª—å­RLé¡¹ç›®ç»„*
### è®­ç»ƒï¼ˆTorch + SB3ï¼‰

```bash
pip install "stable-baselines3[extra]" torch gymnasium tensorboard

# è®­ç»ƒï¼ˆè‡ªåŠ¨é€‰æ‹© cuda/mps/cpuï¼‰
python -m rl_specialized.training.train_specialized --num_players 2 --timesteps 200000

# TensorBoard
tensorboard --logdir runs/rl_specialized
```

å†…éƒ¨é›†æˆï¼š
- å•æ™ºèƒ½ä½“ Gym åŒ…è£…ï¼ˆRL æ§åˆ¶ player_0ï¼Œå…¶ä½™ä¸º BasicRuleAgentï¼‰
- è‡ªå®šä¹‰ç­–ç•¥ Maskï¼šåœ¨ logits å¤„å±è”½éæ³•åŠ¨ä½œï¼ˆåŠ¨ä½œæ©ç ï¼‰
- ç‰¹å¾æå–å™¨é‡‡ç”¨ `MaskedStateFeatureExtractor`ï¼Œé»˜è®¤ `features_dim=128`

å¦‚éœ€è‡ªå®šä¹‰ç½‘ç»œå®½åº¦ï¼Œå¯åœ¨ `train_specialized.py` ä¸­è°ƒæ•´ï¼š

```python
from rl_specialized.networks.policy_network import make_default_policy_kwargs, PolicyNetConfig

policy_kwargs = make_default_policy_kwargs(PolicyNetConfig(features_dim=128, pi_hidden=128, vf_hidden=128))
model = PPO(MaskedActorCriticPolicy, env, policy_kwargs=policy_kwargs, ...)
```

### è‡ªåšå¼ˆè®­ç»ƒï¼ˆæ··åˆå¯¹æ‰‹æ± ï¼Œç®€åŒ–å®ç°ï¼‰

- æ€è·¯ï¼šRL æ§åˆ¶ player_0ï¼Œå¯¹æ‰‹æ¥è‡ªâ€œå¯¹æ‰‹æ± â€ï¼ˆå¤šå‚æ•°è§„åˆ™ä½“ + å†å²ç­–ç•¥å¿«ç…§ï¼‰ã€‚è®­ç»ƒä¸­æŒ‰è¿›åº¦çº¿æ€§é™ä½è§„åˆ™ä½“å æ¯”ï¼Œå®šæœŸå°†å½“å‰ç­–ç•¥å¿«ç…§åŠ å…¥å¯¹æ‰‹æ± ã€‚
- æ–‡ä»¶ï¼š`rl_specialized/training/env_wrappers.py`ï¼ˆè‡ªåšå¼ˆç¯å¢ƒä¸å¯¹æ‰‹æ± ï¼‰+ `rl_specialized/training/train_selfplay.py`ï¼ˆè®­ç»ƒè„šæœ¬ï¼‰ã€‚

è¿è¡Œï¼š

```bash
python -m rl_specialized.training.train_selfplay --num_players 2 --timesteps 2000000 --snapshot_freq 200000
```

è¦ç‚¹ï¼š
- å¯¹æ‰‹æ± åˆå§‹åŒ…å«å¤šå‚æ•°è§„åˆ™å¯¹æ‰‹ï¼ˆèµ·æ‰‹é¢å€¼âˆˆ{3,4,5}ï¼ŒæŒ‘æˆ˜é˜ˆå€¼åç§»âˆˆ{2,3,4,5}ï¼‰ã€‚
- è§„åˆ™ä½“å æ¯”ä» 0.8 â†’ 0.2ï¼ˆçº¿æ€§éšè®­ç»ƒè¿›åº¦ä¸‹é™ï¼‰ã€‚
- æ¯ `snapshot_freq` æ­¥å°†å½“å‰ç­–ç•¥ä¿å­˜å¹¶åŠ å…¥å¯¹æ‰‹æ± ï¼ˆæ¨æ–­åœ¨ CPU ä¸Šè¿›è¡Œï¼Œä¸å ç”¨è®­ç»ƒè®¾å¤‡æ˜¾å­˜ï¼‰ã€‚
- è§‚æµ‹ä¸åŠ¨ä½œæ©ç ä¸ä¸“ç”¨è®­ç»ƒä¸€è‡´ï¼š`{'obs': state_vec, 'action_mask': mask}`ã€‚

#### å¥–åŠ±æ½œåœ¨å¡‘å½¢ï¼ˆé»˜è®¤å¼€å¯ï¼‰

- ç›®çš„ï¼šåœ¨ä¸æ”¹å˜æœ€ä¼˜ç­–ç•¥çš„å‰æä¸‹ï¼Œæä¾›æ›´å¯†é›†çš„å­¦ä¹ ä¿¡å·ï¼Œæå‡ç¨³å®šæ€§ä¸æ”¶æ•›é€Ÿåº¦ã€‚
- å½¢å¼ï¼šåœ¨è‡ªåšå¼ˆç¯å¢ƒä¸­åŠ å…¥åŠ¿å‡½æ•°å¡‘å½¢é¡¹

  `F = Î² Â· (Î³ Â· Î¦(s') âˆ’ Î¦(s))`

  å…¶ä¸­ Î¦(s) ä¼°è®¡å½“å‰æœ€åå«ç‚¹ç›¸å¯¹äºâ€œå¯ä¿¡åº¦â€çš„å·®è·ï¼š
  - é£ï¼šæˆåŠŸæ¦‚ç‡ p(face æˆ– 1) = 2/6ï¼›æ–‹ï¼šp(face) = 1/6
  - æœŸæœ›æ€»æˆåŠŸ E = è‡ªå·±æ‰‹ç‰ŒæˆåŠŸæ•° + æœªçŸ¥éª°å­æ•° Ã— p
  - Î¦(s) = clip((E âˆ’ æœ€åå«ç‚¹count)/æ€»éª°å­æ•°, âˆ’1, 1)

- å‚æ•°ï¼šåœ¨ `train_selfplay.py` ä¸­é»˜è®¤ `Î²=0.05`ï¼Œ`Î³` ä¸ PPO çš„ `gamma` ä¿æŒä¸€è‡´ï¼ˆé»˜è®¤ 0.99ï¼‰ã€‚
- å…³é—­æ–¹å¼ï¼šå¦‚éœ€å…³é—­ï¼Œåœ¨åˆ›å»º `LiarDiceSelfPlayEnv` æ—¶å°† `dense_shaping=False`ã€‚
