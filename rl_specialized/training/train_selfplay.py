import os
import sys
from dataclasses import dataclass
from typing import Optional, Callable

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ° Python è·¯å¾„
project_root = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', '..'))
if project_root not in sys.path:
    sys.path.insert(0, project_root)

import numpy as np
import torch as th
from stable_baselines3 import PPO
from stable_baselines3.common.callbacks import BaseCallback, EvalCallback
from stable_baselines3.common.vec_env import DummyVecEnv, VecNormalize

from rl_specialized.training.env_wrappers import OpponentPool, LiarDiceSelfPlayEnv
from rl_specialized.training.masked_policy import MaskedActorCriticPolicy
from rl_specialized.networks.policy_network import make_default_policy_kwargs, PolicyNetConfig


def auto_select_device() -> str:
    if th.cuda.is_available():
        return "cuda"
    try:
        if hasattr(th.backends, "mps") and th.backends.mps.is_available():
            return "mps"
    except Exception:
        pass
    return "cpu"


@dataclass
class SelfPlayConfig:
    num_players: int = 2
    dice_per_player: int = 5
    total_timesteps: int = 50_000  # å¯é€šè¿‡å‘½ä»¤è¡Œè¦†ç›–
    # å­¦ä¹ ç‡ä¸æ›´æ–°è§„æ¨¡ä¼˜åŒ–
    learning_rate: float = 2e-4
    learning_rate_end: float = 5e-5
    policy_hidden_size: int = 256
    n_steps: int = 4096
    n_epochs: int = 6
    batch_size: int = 512
    gamma: float = 0.98
    gae_lambda: float = 0.95
    clip_range: float = 0.2
    clip_range_end: float = 0.1
    ent_coef_start: float = 0.02
    ent_coef_end: float = 0.005
    vf_coef: float = 0.7
    target_kl: float = 0.05
    seed: Optional[int] = 42
    device: Optional[str] = None
    tensorboard_log: Optional[str] = "runs/rl_selfplay"
    eval_episodes: int = 20
    snapshot_freq: int = 2_000
    rule_ratio_start: float = 0.8
    rule_ratio_end: float = 0.02


def linear_schedule(start: float, end: float) -> Callable[[float], float]:
    """SB3 å…¼å®¹çš„çº¿æ€§è°ƒåº¦ï¼šprogress_remaining âˆˆ [0,1]ã€‚
    å€¼éšè®­ç»ƒè¿›åº¦ä» start â†’ end çº¿æ€§å˜åŒ–ã€‚
    """
    start = float(start)
    end = float(end)
    def fn(progress_remaining: float) -> float:
        return end + (start - end) * float(progress_remaining)
    return fn


class SelfPlayCallback(BaseCallback):
    """æŒ‰è¿›åº¦æ›´æ–°å¯¹æ‰‹æ± å æ¯”ï¼Œå¹¶å®šæœŸåŠ å…¥ç­–ç•¥å¿«ç…§"""

    def __init__(self, pool: OpponentPool, cfg: SelfPlayConfig, save_dir: str):
        super().__init__()
        self.pool = pool
        self.cfg = cfg
        self.save_dir = save_dir
        self._next_snapshot = cfg.snapshot_freq
        self._log_interval = 2_000
        self._diagnostic_interval = 20_000
        self._next_diagnostic = self._diagnostic_interval

        # è¯Šæ–­ç”¨çš„å†å²æ•°æ®
        self._reward_history = []
        self._ep_len_history = []
        self._loss_history = []
        self._explained_var_history = []

    def _on_step(self) -> bool:
        # æ›´æ–°è§„åˆ™ä½“å æ¯”ï¼šçº¿æ€§ä» start åˆ° end
        progress = min(1.0, self.num_timesteps / max(1, self.cfg.total_timesteps))
        ratio = (1 - progress) * self.cfg.rule_ratio_start + progress * self.cfg.rule_ratio_end
        self.pool.set_rule_ratio(ratio)

        # ç†µç³»æ•°é€€ç«ï¼šä» ent_coef_start â†’ ent_coef_end
        try:
            if hasattr(self.model, "ent_coef"):
                ent = (1 - progress) * self.cfg.ent_coef_start + progress * self.cfg.ent_coef_end
                value = float(ent)
                self.model.ent_coef = value
                if hasattr(self.model, "ent_coef_tensor"):
                    self.model.ent_coef_tensor = th.tensor(value, device=self.model.device)
        except Exception:
            pass

        # å®šæœŸè¾“å‡ºè®­ç»ƒæ‘˜è¦
        if self.num_timesteps % self._log_interval == 0 or self.num_timesteps == 1:
            # æ”¶é›†æŒ‡æ ‡
            self._collect_metrics()

            stats = self.pool.get_usage_stats()
            pool_summary = self.pool.get_pool_summary()
            print(f"è®­ç»ƒæ­¥æ•°: {self.num_timesteps:,}/{self.cfg.total_timesteps:,} (è¿›åº¦ {progress*100:.1f}%), è§„åˆ™å¯¹æ‰‹å æ¯”: {ratio:.2f}")
            print(f"å¯¹æ‰‹æ± : {pool_summary}")
            if sum(stats.values()) > 0:
                print(
                    "ä½¿ç”¨ç»Ÿè®¡: åŸºç¡€è§„åˆ™{basic:.0f}% | æ¦‚ç‡è§„åˆ™{prob:.0f}% | ç­–ç•¥å¿«ç…§{policy:.0f}%".format(
                        basic=stats.get('basic_rule', 0.0),
                        prob=stats.get('prob_rule', 0.0),
                        policy=stats.get('policy', 0.0),
                    )
                )
            print("å½“å‰ç†µç³»æ•°: {:.4f}".format(getattr(self.model, "ent_coef", 0.0)))
            print("-" * 60)
            self.pool.reset_stats()

        # åˆ°è¾¾å¿«ç…§ç‚¹ï¼šä¿å­˜å¹¶æ³¨å†Œä¸ºå¯¹æ‰‹
        if self.num_timesteps >= self._next_snapshot:
            print(f"ä¿å­˜å¿«ç…§: {self.num_timesteps} æ­¥")
            os.makedirs(self.save_dir, exist_ok=True)
            path = os.path.join(self.save_dir, f"snapshot_step_{self.num_timesteps}.zip")
            self.model.save(path)
            # ç­–ç•¥å¯¹æ‰‹æ¨æ–­æ”¾CPU
            self.pool.add_policy(path, device="cpu")
            print(f"å·²ä¿å­˜å¿«ç…§: {path}")
            self._next_snapshot += self.cfg.snapshot_freq

        # åˆ°è¾¾è¯Šæ–­ç‚¹ï¼šè¾“å‡ºè®­ç»ƒçŠ¶æ€è¯Šæ–­
        if self.num_timesteps >= self._next_diagnostic:
            self._print_diagnostic(progress)
            self._next_diagnostic += self._diagnostic_interval

        return True

    def _on_rollout_end(self) -> None:
        """åœ¨æ¯æ¬¡ rollout ç»“æŸæ—¶æ”¶é›†æŒ‡æ ‡ï¼ˆæ­¤æ—¶ logger å·²æ›´æ–°ï¼‰"""
        self._collect_metrics()

    def _collect_metrics(self):
        """æ”¶é›†å½“å‰è®­ç»ƒæŒ‡æ ‡ç”¨äºè¯Šæ–­"""
        # ä» locals å’Œ logger æ”¶é›†æŒ‡æ ‡ï¼ˆSB3 å›è°ƒæœºåˆ¶ï¼‰
        try:
            # æ–¹æ³•1: ä» self.locals è·å–ï¼ˆåœ¨è®­ç»ƒå¾ªç¯ä¸­å¯ç”¨ï¼‰
            if hasattr(self, 'locals') and self.locals:
                # å›æŠ¥å’Œå›åˆé•¿åº¦ä» rollout buffer çš„ info è·å–
                if 'infos' in self.locals:
                    infos = self.locals['infos']
                    for info in infos:
                        if isinstance(info, dict):
                            if 'episode' in info:
                                ep_info = info['episode']
                                if 'r' in ep_info:
                                    self._reward_history.append(float(ep_info['r']))
                                if 'l' in ep_info:
                                    self._ep_len_history.append(float(ep_info['l']))

            # æ–¹æ³•2: ä» logger çš„ name_to_value å­—å…¸è·å–
            if hasattr(self.logger, 'name_to_value'):
                metrics = self.logger.name_to_value
                if 'rollout/ep_rew_mean' in metrics:
                    self._reward_history.append(float(metrics['rollout/ep_rew_mean']))
                if 'rollout/ep_len_mean' in metrics:
                    self._ep_len_history.append(float(metrics['rollout/ep_len_mean']))
                if 'train/loss' in metrics:
                    self._loss_history.append(float(metrics['train/loss']))
                if 'train/explained_variance' in metrics:
                    self._explained_var_history.append(float(metrics['train/explained_variance']))

            # æ–¹æ³•3: ä» logger çš„ name_to_value å±æ€§ï¼ˆSB3 æ–°ç‰ˆæœ¬ï¼‰
            elif hasattr(self.logger, 'get_log_dict'):
                metrics = self.logger.get_log_dict()
                if 'rollout/ep_rew_mean' in metrics:
                    self._reward_history.append(float(metrics['rollout/ep_rew_mean']))
                if 'rollout/ep_len_mean' in metrics:
                    self._ep_len_history.append(float(metrics['rollout/ep_len_mean']))
                if 'train/loss' in metrics:
                    self._loss_history.append(float(metrics['train/loss']))
                if 'train/explained_variance' in metrics:
                    self._explained_var_history.append(float(metrics['train/explained_variance']))
        except Exception as e:
            # é™é»˜å¤±è´¥ï¼Œä¸å½±å“è®­ç»ƒ
            pass

    def _print_diagnostic(self, progress: float):
        """è¾“å‡ºè¯¦ç»†çš„è®­ç»ƒè¯Šæ–­ä¿¡æ¯"""
        self._collect_metrics()

        print("\n" + "=" * 80)
        print(f"ğŸ”¬ è®­ç»ƒè¯Šæ–­æŠ¥å‘Š - æ­¥æ•°: {self.num_timesteps:,} / {self.cfg.total_timesteps:,} (è¿›åº¦ {progress*100:.1f}%)")
        print("=" * 80)

        # 1. æ€§èƒ½è¯„ä¼°
        print("\nğŸ“Š ã€æ€§èƒ½è¯„ä¼°ã€‘")

        # å°è¯•ä»å¤šä¸ªæ¥æºè·å–å›æŠ¥æ•°æ®
        avg_reward = None
        reward_std = None

        if self._reward_history:
            recent_rewards = self._reward_history[-10:]
            avg_reward = np.mean(recent_rewards)
            reward_std = np.std(recent_rewards)
            reward_trend = "ä¸Šå‡" if len(recent_rewards) > 1 and recent_rewards[-1] > recent_rewards[0] else "ä¸‹é™"

            print(f"  â€¢ å¹³å‡å›æŠ¥: {avg_reward:.3f} (std: {reward_std:.3f})")
            print(f"  â€¢ å›æŠ¥è¶‹åŠ¿: {reward_trend}")
        else:
            # å¤‡ç”¨æ–¹æ¡ˆï¼šä» logger ç›´æ¥è¯»å–æœ€æ–°å€¼
            try:
                if hasattr(self.logger, 'name_to_value'):
                    metrics = self.logger.name_to_value
                    if 'rollout/ep_rew_mean' in metrics:
                        avg_reward = float(metrics['rollout/ep_rew_mean'])
                        print(f"  â€¢ å½“å‰å¹³å‡å›æŠ¥: {avg_reward:.3f}")
                    else:
                        print("  â€¢ å›æŠ¥æ•°æ®: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸæˆ–é‡‡æ ·ä¸­ï¼‰")
                else:
                    print("  â€¢ å›æŠ¥æ•°æ®: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸæˆ–é‡‡æ ·ä¸­ï¼‰")
            except:
                print("  â€¢ å›æŠ¥æ•°æ®: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸæˆ–é‡‡æ ·ä¸­ï¼‰")

        # æ€§èƒ½è¯„çº§
        if avg_reward is not None:
            if avg_reward > 0.5:
                status = "âœ… ä¼˜ç§€"
            elif avg_reward > 0.0:
                status = "âœ“ è‰¯å¥½"
            elif avg_reward > -0.5:
                status = "âš ï¸ ä¸€èˆ¬"
            else:
                status = "âŒ è¾ƒå·®"
            print(f"  â€¢ æ€§èƒ½ç­‰çº§: {status}")

        # 2. ç¨³å®šæ€§åˆ†æ
        print("\nğŸ¯ ã€ç¨³å®šæ€§åˆ†æã€‘")

        loss_mean = None
        if self._loss_history:
            recent_loss = self._loss_history[-10:]
            loss_std = np.std(recent_loss)
            loss_mean = np.mean(recent_loss)

            print(f"  â€¢ å¹³å‡æŸå¤±: {loss_mean:.4f} (std: {loss_std:.4f})")
            if loss_std < 0.1:
                print("  â€¢ æŸå¤±ç¨³å®šæ€§: âœ… ç¨³å®š")
            elif loss_std < 0.3:
                print("  â€¢ æŸå¤±ç¨³å®šæ€§: âš ï¸ è½»å¾®æ³¢åŠ¨")
            else:
                print("  â€¢ æŸå¤±ç¨³å®šæ€§: âŒ æ³¢åŠ¨è¾ƒå¤§")
        else:
            try:
                if hasattr(self.logger, 'name_to_value'):
                    metrics = self.logger.name_to_value
                    if 'train/loss' in metrics:
                        loss_mean = float(metrics['train/loss'])
                        print(f"  â€¢ å½“å‰æŸå¤±: {loss_mean:.4f}")
                    else:
                        print("  â€¢ æŸå¤±æ•°æ®: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸï¼‰")
                else:
                    print("  â€¢ æŸå¤±æ•°æ®: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸï¼‰")
            except:
                print("  â€¢ æŸå¤±æ•°æ®: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸï¼‰")

        ev_mean = None
        if self._explained_var_history:
            recent_ev = self._explained_var_history[-10:]
            ev_mean = np.mean(recent_ev)

            print(f"  â€¢ è§£é‡Šæ–¹å·®: {ev_mean:.3f}")
        else:
            try:
                if hasattr(self.logger, 'name_to_value'):
                    metrics = self.logger.name_to_value
                    if 'train/explained_variance' in metrics:
                        ev_mean = float(metrics['train/explained_variance'])
                        print(f"  â€¢ å½“å‰è§£é‡Šæ–¹å·®: {ev_mean:.3f}")
                    else:
                        print("  â€¢ è§£é‡Šæ–¹å·®: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸï¼‰")
                else:
                    print("  â€¢ è§£é‡Šæ–¹å·®: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸï¼‰")
            except:
                print("  â€¢ è§£é‡Šæ–¹å·®: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸï¼‰")

        # å€¼å‡½æ•°è´¨é‡è¯„ä¼°
        if ev_mean is not None:
            if ev_mean > 0.5:
                print("  â€¢ å€¼å‡½æ•°è´¨é‡: âœ… ä¼˜ç§€")
            elif ev_mean > 0.2:
                print("  â€¢ å€¼å‡½æ•°è´¨é‡: âœ“ è‰¯å¥½")
            elif ev_mean > 0:
                print("  â€¢ å€¼å‡½æ•°è´¨é‡: âš ï¸ ä¸€èˆ¬ï¼ˆå¯èƒ½éœ€è¦è°ƒæ•´ vf_coef æˆ– n_stepsï¼‰")
            else:
                print("  â€¢ å€¼å‡½æ•°è´¨é‡: âŒ è¾ƒå·®ï¼ˆå»ºè®®æ£€æŸ¥å¥–åŠ±å¡‘å½¢å’ŒçŠ¶æ€è¡¨ç¤ºï¼‰")

        # 3. è¡Œä¸ºåˆ†æ
        print("\nğŸ® ã€ç­–ç•¥è¡Œä¸ºã€‘")

        avg_len = None
        if self._ep_len_history:
            recent_len = self._ep_len_history[-10:]
            avg_len = np.mean(recent_len)
            print(f"  â€¢ å¹³å‡å›åˆé•¿åº¦: {avg_len:.2f} æ­¥")
        else:
            try:
                if hasattr(self.logger, 'name_to_value'):
                    metrics = self.logger.name_to_value
                    if 'rollout/ep_len_mean' in metrics:
                        avg_len = float(metrics['rollout/ep_len_mean'])
                        print(f"  â€¢ å½“å‰å›åˆé•¿åº¦: {avg_len:.2f} æ­¥")
                    else:
                        print("  â€¢ å›åˆé•¿åº¦: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸï¼‰")
                else:
                    print("  â€¢ å›åˆé•¿åº¦: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸï¼‰")
            except:
                print("  â€¢ å›åˆé•¿åº¦: æš‚æ— ï¼ˆè®­ç»ƒåˆæœŸï¼‰")

        if avg_len is not None:
            expected_len = 2.5 + (self.cfg.num_players - 2) * 0.5

            if avg_len < 1.8:
                print("  â€¢ ç­–ç•¥ç±»å‹: âš ï¸ è¿‡äºä¿å®ˆï¼ˆé¢‘ç¹æŒ‘æˆ˜ï¼‰")
            elif avg_len < expected_len * 0.8:
                print("  â€¢ ç­–ç•¥ç±»å‹: åä¿å®ˆ")
            elif avg_len > expected_len * 1.3:
                print("  â€¢ ç­–ç•¥ç±»å‹: åæ¿€è¿›ï¼ˆå¯èƒ½è¿‡åº¦å«ç‰Œï¼‰")
            else:
                print("  â€¢ ç­–ç•¥ç±»å‹: âœ“ å‡è¡¡")

        # 4. æ¢ç´¢çŠ¶æ€
        print("\nğŸ” ã€æ¢ç´¢çŠ¶æ€ã€‘")
        current_ent = getattr(self.model, "ent_coef", 0.0)
        print(f"  â€¢ å½“å‰ç†µç³»æ•°: {current_ent:.4f}")
        if current_ent > 0.01:
            print("  â€¢ æ¢ç´¢ç¨‹åº¦: ğŸ”¥ ç§¯ææ¢ç´¢ä¸­")
        elif current_ent > 0.005:
            print("  â€¢ æ¢ç´¢ç¨‹åº¦: âœ“ é€‚åº¦æ¢ç´¢")
        else:
            print("  â€¢ æ¢ç´¢ç¨‹åº¦: â„ï¸ æ”¶æ•›é˜¶æ®µï¼ˆä½æ¢ç´¢ï¼‰")

        # 5. å¯¹æ‰‹æ± çŠ¶æ€
        print("\nğŸ‘¥ ã€å¯¹æ‰‹æ± çŠ¶æ€ã€‘")
        stats = self.pool.get_usage_stats()
        pool_summary = self.pool.get_pool_summary()
        print(f"  â€¢ å¯¹æ‰‹æ± é…ç½®: {pool_summary}")
        if sum(stats.values()) > 0:
            print(f"  â€¢ è§„åˆ™å¯¹æ‰‹: {stats.get('basic_rule', 0.0) + stats.get('prob_rule', 0.0):.0f}%")
            print(f"  â€¢ ç­–ç•¥å¿«ç…§: {stats.get('policy', 0.0):.0f}%")

        # 6. å»ºè®®
        print("\nğŸ’¡ ã€è®­ç»ƒå»ºè®®ã€‘")
        issues = []

        if self._reward_history and np.mean(self._reward_history[-10:]) < -0.5:
            issues.append("â€¢ å›æŠ¥åä½ï¼šè€ƒè™‘æ£€æŸ¥å¥–åŠ±å¡‘å½¢ç³»æ•°æˆ–å¢åŠ æ¢ç´¢")

        if self._explained_var_history and np.mean(self._explained_var_history[-10:]) < 0.1:
            issues.append("â€¢ å€¼å‡½æ•°è´¨é‡å·®ï¼šå»ºè®®æé«˜ vf_coef (å½“å‰ {:.1f}) æˆ–å¢å¤§ n_steps".format(self.cfg.vf_coef))

        if self._ep_len_history and np.mean(self._ep_len_history[-10:]) < 1.8:
            issues.append("â€¢ å›åˆè¿‡çŸ­ï¼šç­–ç•¥è¿‡äºä¿å®ˆï¼Œå¯èƒ½éœ€è¦é™ä½æŒ‘æˆ˜å¥–åŠ±")

        if self._loss_history and np.std(self._loss_history[-10:]) > 0.5:
            issues.append("â€¢ è®­ç»ƒä¸ç¨³å®šï¼šè€ƒè™‘é™ä½å­¦ä¹ ç‡æˆ–å‡å° target_kl")

        if not issues:
            print("  âœ… è®­ç»ƒçŠ¶æ€è‰¯å¥½ï¼Œç»§ç»­ä¿æŒï¼")
        else:
            for issue in issues:
                print(f"  {issue}")

        print("\n" + "=" * 80 + "\n")


class SelfPlayTrainer:
    def __init__(self, cfg: SelfPlayConfig):
        self.cfg = cfg
        if self.cfg.device is None:
            self.cfg.device = auto_select_device()

        # æ„å»ºå¯¹æ‰‹æ± ï¼šåˆå§‹åŒ–è‹¥å¹²è§„åˆ™å˜ä½“ï¼ˆåŸºç¡€ + æ¦‚ç‡å‹ï¼‰
        self.pool = OpponentPool(num_players=cfg.num_players, rule_ratio=cfg.rule_ratio_start)
        # åŸºç¡€è§„åˆ™å¯¹æ‰‹ï¼šæŒ‘æˆ˜é˜ˆå€¼åç§» 2..5ï¼Œèµ·æ‰‹é¢å€¼ 3/4/5
        for off in [2, 3, 4, 5]:
            for face in [3, 4, 5]:
                self.pool.add_rule(start_face=face, challenge_offset=off)
        # æ¦‚ç‡å‹è§„åˆ™å¯¹æ‰‹ï¼šä¸åŒé˜ˆå€¼ç»„åˆï¼Œæå‡å¯¹æ‰‹å¤šæ ·æ€§
        for tc in [0.20, 0.25, 0.30]:
            for tr in [0.55, 0.60, 0.65]:
                self.pool.add_prob_rule(theta_challenge=tc, target_raise=tr, max_extra_raise=2)

        def make_env():
            # é»˜è®¤å¼€å¯æ½œåœ¨å¡‘å½¢ï¼šÎ²=0.05ï¼ŒÎ³åŒ PPO é…ç½®ï¼ˆcfg.gammaï¼‰
            return LiarDiceSelfPlayEnv(
                pool=self.pool,
                dice_per_player=cfg.dice_per_player,
                dense_shaping=True,
                shaping_beta=0.05,
                shaping_gamma=cfg.gamma,
                show_opponent_info=False,  # åœ¨è®­ç»ƒæ—¶ä¸æ˜¾ç¤ºè¯¦ç»†å¯¹æ‰‹ä¿¡æ¯
            )

        from stable_baselines3.common.monitor import Monitor

        def make_env_with_monitor():
            env = make_env()
            return Monitor(env)  # æ·»åŠ MonitoråŒ…è£…è§£å†³è­¦å‘Š

        # å½’ä¸€åŒ–åŒ…è£…ï¼šè®­ç»ƒæœŸä»…å½’ä¸€åŒ–è§‚æµ‹ï¼Œä¸å½’ä¸€åŒ–å¥–åŠ±ï¼›è¯„ä¼°ä»…ä½¿ç”¨ç»Ÿè®¡ä¸æ›´æ–°
        self.train_env = VecNormalize(
            DummyVecEnv([make_env_with_monitor]), norm_obs=True, norm_reward=False, clip_obs=10.0,
            norm_obs_keys=["obs"]  # åªå½’ä¸€åŒ– obs éƒ¨åˆ†ï¼Œä¸å½’ä¸€åŒ– action_mask
        )
        self.eval_env = VecNormalize(
            DummyVecEnv([make_env_with_monitor]), norm_obs=True, norm_reward=False, clip_obs=10.0,
            norm_obs_keys=["obs"]  # åªå½’ä¸€åŒ– obs éƒ¨åˆ†ï¼Œä¸å½’ä¸€åŒ– action_mask
        )
        # å…±äº«ç»Ÿè®¡å¹¶å…³é—­è¯„ä¼°æœŸç»Ÿè®¡æ›´æ–°
        self.eval_env.obs_rms = self.train_env.obs_rms
        self.eval_env.ret_rms = self.train_env.ret_rms
        self.eval_env.training = False

        policy_kwargs = make_default_policy_kwargs(PolicyNetConfig(
            features_dim=cfg.policy_hidden_size,
            pi_hidden=cfg.policy_hidden_size,
            vf_hidden=cfg.policy_hidden_size,
        ))

        self.model = PPO(
            policy=MaskedActorCriticPolicy,
            env=self.train_env,
            learning_rate=linear_schedule(cfg.learning_rate, cfg.learning_rate_end),
            n_steps=cfg.n_steps,
            n_epochs=cfg.n_epochs,
            batch_size=cfg.batch_size,
            gamma=cfg.gamma,
            gae_lambda=cfg.gae_lambda,
            clip_range=linear_schedule(cfg.clip_range, cfg.clip_range_end),
            clip_range_vf=1.0,
            ent_coef=cfg.ent_coef_start,
            vf_coef=cfg.vf_coef,
            target_kl=cfg.target_kl,
            tensorboard_log=cfg.tensorboard_log,
            policy_kwargs=policy_kwargs,
            seed=cfg.seed,
            device=cfg.device,
            verbose=1,  # æ·»åŠ è¯¦ç»†è¾“å‡º
        )

    def train(self):
        print(f"å¼€å§‹è®­ç»ƒ - æ€»æ­¥æ•°: {self.cfg.total_timesteps}, ç©å®¶æ•°: {self.cfg.num_players}, è®¾å¤‡: {self.cfg.device}")
        print(f"ç½‘ç»œé…ç½® - éšè—å±‚å¤§å°: {self.cfg.policy_hidden_size}, å­¦ä¹ ç‡: {self.cfg.learning_rate}")
        print(f"PPOå‚æ•° - n_steps: {self.cfg.n_steps}, n_epochs: {self.cfg.n_epochs}, batch_size: {self.cfg.batch_size}")
        print(f"å¯¹æ‰‹æ± åˆå§‹é…ç½®: {self.pool.get_pool_summary()}")
        print(f"è§„åˆ™å¯¹æ‰‹å æ¯”: {self.cfg.rule_ratio_start:.1f} â†’ {self.cfg.rule_ratio_end:.1f}")
        print("-" * 60)

        save_dir = os.path.join(self.cfg.tensorboard_log or "runs", "snapshots")
        sp_cb = SelfPlayCallback(pool=self.pool, cfg=self.cfg, save_dir=save_dir)
        eval_cb = EvalCallback(self.eval_env, best_model_save_path=os.path.join(self.cfg.tensorboard_log or "runs", "best_model"),
                               log_path=self.cfg.tensorboard_log, eval_freq=max(2000, self.cfg.n_steps),  # æ›´é¢‘ç¹çš„è¯„ä¼°
                               deterministic=False, render=False, n_eval_episodes=self.cfg.eval_episodes)

        print("å¼€å§‹ PPO è®­ç»ƒ...")
        self.model.learn(total_timesteps=self.cfg.total_timesteps, callback=[sp_cb, eval_cb])
        print("è®­ç»ƒå®Œæˆ!")

    def evaluate(self, n_episodes: Optional[int] = None) -> float:
        if n_episodes is None:
            n_episodes = self.cfg.eval_episodes
        env = self.eval_env
        total_reward = 0.0
        for _ in range(n_episodes):
            obs = env.reset()
            done = False
            while not done:
                action, _ = self.model.predict(obs, deterministic=False)
                obs, reward, done, info = env.step(action)
                done = bool(done[0])
                total_reward += float(reward[0])
        return total_reward / n_episodes


def main():
    import argparse
    parser = argparse.ArgumentParser()
    parser.add_argument("--num_players", type=int, default=2)
    parser.add_argument("--dice_per_player", type=int, default=5)
    parser.add_argument("--timesteps", type=int, default=2_000_000)
    parser.add_argument("--snapshot_freq", type=int, default=20_000)
    parser.add_argument("--device", type=str, default=None, choices=["cuda", "mps", "cpu", None])
    parser.add_argument("--logdir", type=str, default="runs/rl_selfplay")
    parser.add_argument("--seed", type=int, default=42)
    args = parser.parse_args()

    cfg = SelfPlayConfig(num_players=args.num_players, dice_per_player=args.dice_per_player,
                         total_timesteps=args.timesteps, snapshot_freq=args.snapshot_freq,
                         device=args.device, tensorboard_log=args.logdir, seed=args.seed)

    trainer = SelfPlayTrainer(cfg)
    print(f"Using device: {cfg.device}")
    trainer.train()
    avg_reward = trainer.evaluate()
    print(f"Eval avg reward over {cfg.eval_episodes} eps: {avg_reward:.3f}")


if __name__ == "__main__":
    main()
