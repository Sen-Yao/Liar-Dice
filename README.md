# Liar Dice

这是一个基于强化学习完成的 Liar Dice 的 Agent 项目

## 游戏规则

### 基本规则

Liar Dice 是一个回合制桌面游戏，游戏玩家为 2 名及以上，所有玩家绕桌子围成一圈。按逆时针进行游戏。每位玩家始终有 5 颗、六面骰（点数 1–6）。

“整场（Match）”游戏中，每位玩家的起始罚点为 0 点，输一局会获得 1 个罚点（跳开则为乘数，参见「跳开」），罚点数越少，排名越好。在整场游戏中不会出现玩家个数、骰子个数的变动。

整场游戏由多个局（round）组成。一局游戏的定义为，从所有玩家掷骰子到某一个玩家进行「检验」的循环。

每局游戏开始时，每位玩家掷自己的五颗骰子，骰子结果**仅自己可见**。掷骰结束后，玩家按逆时针进入自己的行动回合，未处于行动回合的玩家无法进行任何操作。行动回合中，玩家必须选择其中一种动作，否则无法结束自己的回合：

- 一次合法猜测，游戏继续，进入下一个玩家的回合
- 检验（challenge）上一个玩家的猜测（如果检验，立即翻牌判定并结算本局）


特别地，当前局的首位玩家在其首回合必须以“猜测”开始（不能检验）。因为场上没有可供检测的猜测。

若是整场游戏的第一局游戏，则随机选择一位玩家来作为本局游戏的第一位玩家开始回合。否则选择上一局的输家作为第一位玩家。
#### 猜测

「**猜测**」指的是此玩家根据自己的骰子结果和对场上其他玩家的估计，公开宣告此玩家猜测某个点数在所有玩家的骰子中的**和大于等于某个值**。所有玩家都将听到并记住此猜测。

注意在本游戏中，点数为 1 的骰子可以看成是任意点数。例如 “7 个 4” 是一个猜测，它表示此玩家认为场上所有玩家手里的骰子中，点数为 4 的骰子和点数为 1 的骰子加起来共有 7 个**及以上**。

在基本规则中，猜测的点数不能为 1

猜测可用形式定义为二元组 `(count, face)`，其中：

- `count` 为正整数，即骰子的个数。上限为当前场上骰子总数，即玩家数 × 5
- `face ∈ {2,3,4,5,6}`，即骰子的点数

猜测存在大小之分。例如猜测 1 和 猜测 2 的大小定义如下（考虑对称性，仅介绍大于和等于的情况）：

- 若 `count_1 > count_2`，则猜测 1 大于猜测 2
- 若 `count_1 == count_2`：
	- 若 `face_1 > face_2`，则猜测 1 大于猜测 2
	- 若 `face_1 == face_2`，则猜测 1 等于猜测 2

每一个玩家进行猜测时，要求猜测严格大于上一个玩家。

特别地，为了防止每局游戏的早期猜测过于简单，要求当前局的第一个回合的玩家的猜测，个数 `count` 必须大于当前玩家数。比如对于一局 5 个玩家的游戏，第一个玩家的猜测应从 6 个 X 及以上开始。若该玩家在之后（本局中）再次轮到其回合，不再受该首回合限制。

只有满足以下条件的猜测才是一个合法的猜测。

- `(count, face)` 不超过定义域。
- 对于非首回合的玩家，猜测必须严格大于场上的上一个合法猜测
- 对于首回合玩家，猜测个数 `count` 必须大于当前玩家数。

若玩家做出一个不合法的猜测，则此行动无效，必须立即重新选择合法的猜测或选择对上一个玩家进行检验。

显然，若无合法更大的猜测存在，则当前玩家唯一可选动作为检验

#### 检验

「**检验**」指的是当前回合玩家对上一位玩家的猜测提出了质疑，认为其猜测不合理。当前玩家要求检验时，所有玩家均展示自己的全部骰子情况。并且检验上一位玩家的猜测的真假。

对于上一位玩家的猜测，检验全局所有玩家的骰子结果中，对应点数及点数 1 的骰子个数。

- 若骰子个数大于等于上一位玩家的猜测的个数，则猜测为真。进行检验的玩家输掉此游戏，本局结束。
- 反之猜测为假，则上一个提出猜测的玩家输掉此游戏，本局结束。

一局游戏结束后，上一局输掉的玩家作为新一局游戏的第一位玩家。每局的输家在整场游戏中的罚点数加 1（跳开则为乘数，参见「跳开」）。整场游戏可以在任意一局的结束时，根据玩家意愿结束。结束时根据每位玩家的罚点情况进行结算

### 进阶规则

#### 斋和飞

「斋」和「飞」是一种对基础猜测规则的扩展。具体来说，玩家进行猜测时，可灵活无代价地选择此次猜测适用的规则。

- 「斋」表示不再遵守「点数 1 的骰子可作为任意其他点数」的规则。特别地，在此规则下允许选择 1 作为猜测的点数。
- 「飞」表示仍遵守「点数 1 的骰子可作为任意其他点数」的规则。点数 1 仍被视为万能牌（wild），此时不允许选择 1 作为猜测的点数。

在一局游戏中，玩家可以根据场上局势来在**斋**和**飞**中灵活变化，来选择最有利于自己的猜测方式。猜测可用形式定义为三元组 `(mode, count, face)`，其中：

- `mode ∈ {飞, 斋}`
- `count` 为正整数，即骰子的个数。
- `face`，即骰子的点数。飞规则下定义域为 `{2,3,4,5,6}`，斋规则下定义域为 `{1,2,3,4,5,6}`

引入「斋」和「飞」后，猜测的大小判断同样需要更新。例如猜测 1 和 猜测 2 的大小定义如下（考虑对称性，仅介绍大于和等于的情况）：

- 若 `mode_1 == mode_2`
	- 当双方都是“飞”时：`count_1 > count_2` 则猜测 1 更大；若 `count` 相等，则比较 `face` 的大小（普通 2<3<4<5<6 顺序）。
	- 当双方都是“斋”时：`count_1 > count_2` 则猜测 1 更大；若 `count` 相等，则按 `2 < 3 < 4 < 5 < 6 < 1` 的顺序比较 `face`，其中 1 最大、2 最小。
- 若 `mode_1 == '斋'` 且 `mode_2 == '飞'`
	- 只比较数量，要求 `count_1 ≥ ceil(count_2 / 2)`（等价于 `(count_2 + 1) // 2`）。面值不参与比较。
- 若 `mode_1 == '飞'` 且 `mode_2 == '斋'`
	- 只比较数量，要求 `count_1 ≥ 2 * count_2`。同样忽略面值。

规则比较的权威实现统一封装在 `env.LiarDiceEnv._is_strictly_greater` 中，其他模块（如 `utils.py`、`DQN_agent`）均直接复用该逻辑。

其余猜测规则仍参考基础规则中的猜测。检验时，

- 若被检验的猜测为“飞”模式：计数 = 全场中等于 face 的骰子 + 全场中点数为 1 的骰子（1 为 wild）。
- 若被检验的猜测为“斋”模式：计数 = 全场中等于 face 的骰子（点数为 1 仅计入 face=1 的猜测）。

## Requirements:

```
conda install pytorch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 pytorch-cuda=12.4 -c pytorch -c nvidia
```

## 使用

运行游戏：

```bash
python main.py
```

可选参数：

- `--mode`: 选择 `human` 时，表示作为人类玩家与 AI 对战
- `--render`: 启用渲染显示游戏过程
- `--num_players N`: 设置玩家数量（默认为2）
- `--num_match N`: 设置局数（默认为1）

使用示例：

```bash
# AI训练模式（默认）
python main.py

# 人机对战模式
python main.py --mode human

# 4人对战（1人类+3AI）
python main.py --mode human --num_players 4

# 多场比赛
python main.py --mode human --num_match 3

# 启用详细渲染
python main.py --mode human --render
```

## 最近更新

- 组合动作 Q + 合法动作掩码（更懂规则、少走弯路）
  - 动作是“主动作(猜/检) + 模式(飞/斋) + 数量 + 点数”的组合。现在我们直接对“完整组合”计算/训练一个一致的 Q 值，避免各子头各自为政导致的学习噪声。
  - 用“合法动作掩码”只比较规则允许的动作（比如首手不能检验、飞不能叫1），从源头过滤掉无意义的选择。
  - 搭配 Double DQN（在线网络选、目标网络估）减少过高估计；SmoothL1Loss（Huber）让训练更抗异常值与抖动。

- n-step 回报 + 软目标更新 + 更稳的探索（学得快、学得稳）
  - n=3 的 n-step return 能把终局奖励更快传回到前面几步，在“短局、稀疏奖励”的骗子骰子里尤为重要。
  - 目标网络采用 Polyak 软更新（τ=0.005），目标更平滑；学习率降到 1e-4，并保留梯度裁剪，降低训练振荡。
  - 将最小探索率提升到 0.1，避免太早收敛到“爱挑战/爱抬价”的局部策略，增强泛化。

- 势能奖励整形（替代硬性“前K步禁检验”）
  - 不再强行禁止早期 Challenge，而是根据上次叫点“可信度”构造势能 Φ(s)，用 F=γΦ(s')−Φ(s) 作为微弱 shaping 回报：合理抬价得到鼓励，明显虚高或过早检验被抑制。
  - 这样既延长了对局交互，又显著降低了对人工超参 K 的敏感性。
  - 同时把状态向量扩展到 22 维，加入“上次叫点成立概率”“最近数量变化”“是否切换飞/斋”等趋势特征，帮助模型在前馈网络下更好“记住”上下文。
  - 代码中也预留了 RNN 隐状态接口，后续可无缝升级到 GRU/LSTM 做序列化训练。

## 训练框架与默认设置

- 直接运行 `python train.py` 即可启用上述所有改进版 DQN 流水线；如需自定义，可通过命令行覆盖超参（学习率、n-step、shaping 係数、对手课程等）。
- 模型结构仍为**前馈 MLP 主干 + 四个动作头**（主动作/模式/数量/点数）；虽然 `DQNAgent` 增加了 `hidden_state` 接口，但默认 **未启用 RNN**，除非未来显式加入 `--use_rnn` 开关并替换为 `GRU/LSTM` 主干。
- 状态向量扩展至 22 维，新增“上次叫点成立概率”“最近数量变化”“模式切换标记”等特征，为部分可观测环境提供更丰富的历史线索。

## 非完全马尔可夫性（POMDP）

- 骗子骰子是**部分可观测博弈**：智能体无法看到对手骰子与策略意图，因此严格意义的马尔可夫假设不成立，现有 Q 值估计仍是近似。
- 目前的缓解手段：
  - 强化状态统计：显式编码叫点可信度与模式切换历史。
  - 使用 n-step return + 势能整形，加速终局奖励的回传，减少“短局奖励稀疏”带来的震荡。
  - 对手课程：启发式挑战阈值随训练进程线性提升，冻结自博弈对手刷新得更慢，使样本分布更平滑。
- 即便如此，系统仍假设“扩展状态 ≈ Markov”，在极端策略切换下可能出现值函数震荡，这也是未来引入 RNN 的动力。

## RNN 支持计划

- `DQNAgent` 已预留 `reset_hidden_state` / `detach_hidden_state`，方便未来挂载 `GRU/LSTM` 主干。
- 若启用 RNN，需要：
  1. 重新设计 `ParametricQNetwork` 为循环结构，并在推理时保留隐状态。
  2. 修改回放缓冲，按序列存储并在训练时执行截断 BPTT（例如序列长 16，burn-in 4）。
  3. 调整训练循环与评估逻辑，对每个 episode/round 正确重置隐状态。
- 目前默认流程仍专注在前馈 DQN 的稳定性；如果需要，我可以在后续版本加入 `--use_rnn` 参数并完成完整的序列化训练管线。
