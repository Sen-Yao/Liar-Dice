import argparse
from train.DQN_train import train_dqn
from env import LiarDiceEnv

from agents.DQN_agent import DQNAgent

def main(args):
    """Main training function"""
    # Create environment
    env = LiarDiceEnv(num_players=args.num_players)

    # Create DQN agent
    agent = DQNAgent(agent_id="player_0", num_players=args.num_players, args=args)

    # Train the agent
    trained_agent = train_dqn(agent, env, args)

    print("ğŸ‰ Training completed successfully!")
    return trained_agent

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--num_episodes", type=int, default=1000, help="è®­ç»ƒçš„ episode æ•°")
    parser.add_argument("--max_steps_per_episode", type=int, default=200, help="æ¯ä¸ª episode çš„æœ€å¤§æ¸¸æˆæ­¥")
    parser.add_argument("--batch_size", type=int, default=32, help="ä»ç¼“å†²åŒºæŠ½å–çš„ Batch size")
    parser.add_argument("--gamma", type=float, default=0.99, help="Bellman æ–¹ç¨‹çš„æŠ˜æ‰£ç³»æ•°")
    parser.add_argument("--epsilon_start", type=float, default=1.0, help="å¯åŠ¨æ—¶çš„è´ªå©ª Epsilon")
    parser.add_argument("--epsilon_end", type=float, default=0.01, help="ç»“æŸæ—¶çš„è´ªå©ª Epsilon")
    parser.add_argument("--epsilon_decay", type=float, default=0.995, help="è´ªå©ª Epsilon çš„è¡°å‡ç³»æ•°")
    parser.add_argument("--target_update_freq", type=int, default=100, help="å¤šå°‘ä¸ª episodes æ›´æ–°ä¸€æ¬¡æ¨¡å‹")
    parser.add_argument("--learning_rate", type=float, default=0.001, help="å­¦ä¹ ç‡")

    # Environment settings
    parser.add_argument("--num_players", type=int, default=2, help="ç©å®¶ä¸ªæ•°")

    # Replay buffer
    parser.add_argument("--replay_buffer_size", type=int, default=10000, help="ç¼“å†²åŒºå¤§å°")
    # Model saving
    parser.add_argument("--save_model_freq", type=int, default=100, help="æ¨¡å‹ä¿å­˜é—´éš” episode")
    parser.add_argument("--model_save_path", type=str, default="./models/dqn_model.pth", help="æ¨¡å‹ä¿å­˜è·¯å¾„")

    args = parser.parse_args()

    trained_agent = main(args)