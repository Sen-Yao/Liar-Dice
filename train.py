import argparse
from train.DQN_train import train_dqn
from env import LiarDiceEnv

from agents.DQN_agent import DQNAgent
from pathlib import Path
import json


def str2bool(value):
    if isinstance(value, bool):
        return value
    value = value.strip().lower()
    if value in {"true", "1", "yes", "y", "t"}:
        return True
    if value in {"false", "0", "no", "n", "f"}:
        return False
    raise argparse.ArgumentTypeError(f"æ— æ³•è§£æå¸ƒå°”å€¼: {value}")

def main(args):
    """Main training function"""
    # Create environment
    env = LiarDiceEnv(num_players=args.num_players, history_len=(args.history_len if args.history_len > 0 else None))

    # Create DQN agent
    agent = DQNAgent(agent_id="player_0", num_players=args.num_players, args=args)

    # Train the agent
    trained_agent = train_dqn(agent, env, args)

    print("ğŸ‰ Training completed successfully!")
    return trained_agent

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--num_episodes", type=int, default=1000, help="è®­ç»ƒçš„ episode æ•°")
    parser.add_argument("--max_steps_per_episode", type=int, default=200, help="æ¯ä¸ª episode çš„æœ€å¤§æ¸¸æˆæ­¥ï¼ˆæŒ‰è½®è®¡ï¼‰")
    parser.add_argument("--rounds_per_episode", type=int, default=20, help="æ¯ä¸ª episode é‡ŒåŒ…å«çš„è½®æ•°(round)")
    parser.add_argument("--batch_size", type=int, default=32, help="ä»ç¼“å†²åŒºæŠ½å–çš„ Batch size")
    parser.add_argument("--gamma", type=float, default=0.99, help="Bellman æ–¹ç¨‹çš„æŠ˜æ‰£ç³»æ•°")
    parser.add_argument("--epsilon_start", type=float, default=1.0, help="å¯åŠ¨æ—¶çš„è´ªå©ª Epsilon")
    parser.add_argument("--epsilon_end", type=float, default=0.01, help="ç»“æŸæ—¶çš„è´ªå©ª Epsilon")
    parser.add_argument("--epsilon_decay", type=float, default=0.999, help="æŒ‰æ­¥è¡°å‡ç³»æ•°ï¼ˆæ¯æ­¥ä¹˜ä»¥è¯¥ç³»æ•°ï¼‰")
    parser.add_argument("--target_update_freq", type=int, default=1000, help="æŒ‰æ­¥æ›´æ–° target ç½‘ç»œçš„é¢‘ç‡ï¼ˆstepsï¼‰")
    parser.add_argument("--target_soft_tau", type=float, default=0.005, help="Polyak æ›´æ–°ç³»æ•° Ï„ï¼ˆ>0 å¯ç”¨è½¯æ›´æ–°ï¼‰")
    parser.add_argument("--updates_per_step", type=int, default=1, help="æ¯ä¸ªç¯å¢ƒæ­¥æœŸæœ›æ‰§è¡Œçš„ä¼˜åŒ–æ¬¡æ•°ï¼ˆæŒ‰å›åˆæœ«æˆæ‰¹æ‰§è¡Œï¼‰")
    parser.add_argument("--warmup_steps", type=int, default=3000, help="ä»…æ”¶é›†æ•°æ®çš„é¢„çƒ­æ­¥æ•°ï¼ŒæœŸé—´ä¸è®­ç»ƒ")
    parser.add_argument("--challenge_suppress_steps", type=int, default=0, help="æ¯è½®æœ€å¼€å§‹çš„Kæ­¥æ¢ç´¢æ—¶ç¦æ­¢Challenge")
    parser.add_argument("--learning_rate", type=float, default=0.0001, help="å­¦ä¹ ç‡")

    # Environment settings
    parser.add_argument("--num_players", type=int, default=2, help="ç©å®¶ä¸ªæ•°")
    parser.add_argument("--history_len", type=int, default=0, help="è§‚æµ‹ä¸­ä¿ç•™çš„å†å²é•¿åº¦ï¼ˆ0 è¡¨ç¤ºä¸è£å‰ªï¼‰")

    # Replay buffer
    parser.add_argument("--replay_buffer_size", type=int, default=10000, help="ç¼“å†²åŒºå¤§å°")
    # Model saving
    parser.add_argument("--save_model_freq", type=int, default=100, help="æ¨¡å‹ä¿å­˜é—´éš” episode")
    parser.add_argument("--exp_tag", type=str, default="", help="å®éªŒæ ‡ç­¾ï¼ˆç•™ç©ºåˆ™è‡ªåŠ¨ç”Ÿæˆï¼‰")
    parser.add_argument("--model_save_path", type=str, default="", help="æ¨¡å‹ä¿å­˜è·¯å¾„ï¼ˆç•™ç©ºåˆ™æŒ‰ exp_tag ç”Ÿæˆï¼‰")
    parser.add_argument("--use_wandb", action="store_true", help="å¯ç”¨ wandb æ—¥å¿—è®°å½•ï¼ˆé»˜è®¤å…³é—­ï¼‰")
    parser.add_argument("--mix_mode", choices=["round", "episode"], default="round", help="å¯¹æ‰‹æ··åˆç²’åº¦ï¼ˆå½“ opponent_type=mixed æ—¶ç”Ÿæ•ˆï¼‰")
    parser.add_argument("--heuristic_ratio", type=float, default=0.5, help="æ¯ä¸ªå¸­ä½ä½¿ç”¨å¯å‘å¼å¯¹æ‰‹çš„æ¦‚ç‡ï¼ˆå½“ opponent_type=mixed æ—¶ç”Ÿæ•ˆï¼‰")
    parser.add_argument("--opponent_type", choices=["mixed", "heuristic", "selfplay"], default="mixed", help="å¯¹æ‰‹ç±»å‹ï¼šæ··åˆ/çº¯å¯å‘å¼/çº¯è‡ªåšå¼ˆ")
    parser.add_argument("--frozen_opponent_update_interval", type=int, default=100, help="æ›´æ–°å†»ç»“è‡ªåšå¼ˆå¯¹æ‰‹çš„ episode é—´éš”")
    parser.add_argument("--frozen_pool_size", type=int, default=5, help="ä¿å­˜å¤šå°‘ä¸ªå†»ç»“è‡ªåšå¼ˆå¯¹æ‰‹ç”¨äºè½®æ¢")
    parser.add_argument("--opponent_confidence", type=int, default=2, help="å¯å‘å¼å¯¹æ‰‹çš„æŒ‘æˆ˜æ¿€è¿›ç¨‹åº¦ï¼ˆç»ˆç‚¹ï¼‰")
    parser.add_argument("--opponent_conf_min", type=int, default=1, help="å¯å‘å¼å¯¹æ‰‹æŒ‘æˆ˜æ¿€è¿›åº¦èµ·ç‚¹")
    parser.add_argument("--curriculum_warmup", type=int, default=400, help="å¯¹æ‰‹æ¿€è¿›åº¦çº¿æ€§é€’è¿›æ‰€éœ€ episode æ•°")
    parser.add_argument("--frozen_interval_growth", type=float, default=2.0, help="å†»ç»“å¯¹æ‰‹åˆ·æ–°é—´éš”æ”¾å¤§å€æ•°")
    parser.add_argument("--eval_interval", type=int, default=100, help="è®­ç»ƒä¸­å¿«é€Ÿè¯„æµ‹çš„ episode é—´éš”")
    parser.add_argument("--eval_episodes", type=int, default=100, help="æ¯æ¬¡å¿«é€Ÿè¯„æµ‹çš„å¯¹å±€æ•°")
    parser.add_argument("--quick_eval", type=str2bool, nargs="?", const=True, default=False, help="æ˜¯å¦å¯ç”¨è®­ç»ƒè¿‡ç¨‹ä¸­çš„ quick_evaluateï¼ˆé»˜è®¤å…³é—­ï¼‰")
    parser.add_argument("--min_epsilon", type=float, default=0.1, help="æ¢ç´¢ç‡çš„æœ€å°å€¼")
    parser.add_argument("--epsilon_reset_interval", type=int, default=0, help="å‘¨æœŸæ€§é‡ç½® epsilon çš„ episode é—´éš”ï¼Œä¸º 0 åˆ™ä¸é‡ç½®")
    parser.add_argument("--epsilon_reset_value", type=float, default=0.2, help="é‡ç½®æ—¶çš„ epsilon å€¼")
    parser.add_argument("--n_step", type=int, default=3, help="n-step return çš„æ­¥é•¿")
    parser.add_argument("--shaping_beta", type=float, default=0.2, help="åŠ¿èƒ½å¥–åŠ± shaping ç³»æ•°ï¼ˆ<=0 å…³é—­ï¼‰")
    parser.add_argument("--reward_shaping", type=str2bool, nargs="?", const=True, default=True, help="æ˜¯å¦å¯ç”¨åŠ¿èƒ½å¡‘å½¢ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")
    parser.add_argument("--ddqn", type=str2bool, nargs="?", const=True, default=True, help="æ˜¯å¦å¯ç”¨ Double DQN ç›®æ ‡ï¼ˆé»˜è®¤å¯ç”¨ï¼‰")

    args = parser.parse_args()

    # ç”Ÿæˆé»˜è®¤ exp_tagï¼ˆè‹¥æœªæŒ‡å®šï¼‰
    if not args.exp_tag:
        tag_parts = [
            f"dqn",
            f"ddqn_{1 if args.ddqn else 0}",
            f"shape_{1 if args.reward_shaping and args.shaping_beta > 0 else 0}",
            f"n_{args.n_step}",
            f"hist_{args.history_len if args.history_len > 0 else 0}",
            f"opp_{args.opponent_type}",
        ]
        args.exp_tag = "__".join(tag_parts)

    # ç¡®å®šæ¨¡å‹ä¿å­˜ç›®å½•ä¸è·¯å¾„
    model_dir = Path("models") / args.exp_tag
    model_dir.mkdir(parents=True, exist_ok=True)
    if not args.model_save_path:
        args.model_save_path = str(model_dir / "model.pth")

    # ä¿å­˜è®­ç»ƒé…ç½®ï¼ˆä¾¿äºè¯„ä¼°ç«¯å¤ç°ç¯å¢ƒï¼‰
    cfg = vars(args).copy()
    cfg_path = model_dir / "config.json"
    with open(cfg_path, "w", encoding="utf-8") as f:
        json.dump(cfg, f, ensure_ascii=False, indent=2)

    trained_agent = main(args)
