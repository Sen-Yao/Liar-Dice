import argparse
from train.DQN_train import train_dqn
from env import LiarDiceEnv

from agents.DQN_agent import DQNAgent

def main(args):
    """Main training function"""
    # Create environment
    env = LiarDiceEnv(num_players=args.num_players)

    # Create DQN agent
    agent = DQNAgent(agent_id="player_0", num_players=args.num_players, args=args)

    # Train the agent
    trained_agent = train_dqn(agent, env, args)

    print("ğŸ‰ Training completed successfully!")
    return trained_agent

if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--num_episodes", type=int, default=1000, help="è®­ç»ƒçš„ episode æ•°")
    parser.add_argument("--max_steps_per_episode", type=int, default=200, help="æ¯ä¸ª episode çš„æœ€å¤§æ¸¸æˆæ­¥ï¼ˆæŒ‰è½®è®¡ï¼‰")
    parser.add_argument("--rounds_per_episode", type=int, default=20, help="æ¯ä¸ª episode é‡ŒåŒ…å«çš„è½®æ•°(round)")
    parser.add_argument("--batch_size", type=int, default=32, help="ä»ç¼“å†²åŒºæŠ½å–çš„ Batch size")
    parser.add_argument("--gamma", type=float, default=0.99, help="Bellman æ–¹ç¨‹çš„æŠ˜æ‰£ç³»æ•°")
    parser.add_argument("--epsilon_start", type=float, default=1.0, help="å¯åŠ¨æ—¶çš„è´ªå©ª Epsilon")
    parser.add_argument("--epsilon_end", type=float, default=0.01, help="ç»“æŸæ—¶çš„è´ªå©ª Epsilon")
    parser.add_argument("--epsilon_decay", type=float, default=0.999, help="æŒ‰æ­¥è¡°å‡ç³»æ•°ï¼ˆæ¯æ­¥ä¹˜ä»¥è¯¥ç³»æ•°ï¼‰")
    parser.add_argument("--target_update_freq", type=int, default=1000, help="æŒ‰æ­¥æ›´æ–° target ç½‘ç»œçš„é¢‘ç‡ï¼ˆstepsï¼‰")
    parser.add_argument("--target_soft_tau", type=float, default=0.005, help="Polyak æ›´æ–°ç³»æ•° Ï„ï¼ˆ>0 å¯ç”¨è½¯æ›´æ–°ï¼‰")
    parser.add_argument("--updates_per_step", type=int, default=1, help="æ¯ä¸ªç¯å¢ƒæ­¥æœŸæœ›æ‰§è¡Œçš„ä¼˜åŒ–æ¬¡æ•°ï¼ˆæŒ‰å›åˆæœ«æˆæ‰¹æ‰§è¡Œï¼‰")
    parser.add_argument("--warmup_steps", type=int, default=3000, help="ä»…æ”¶é›†æ•°æ®çš„é¢„çƒ­æ­¥æ•°ï¼ŒæœŸé—´ä¸è®­ç»ƒ")
    parser.add_argument("--challenge_suppress_steps", type=int, default=0, help="æ¯è½®æœ€å¼€å§‹çš„Kæ­¥æ¢ç´¢æ—¶ç¦æ­¢Challenge")
    parser.add_argument("--learning_rate", type=float, default=0.0001, help="å­¦ä¹ ç‡")

    # Environment settings
    parser.add_argument("--num_players", type=int, default=2, help="ç©å®¶ä¸ªæ•°")

    # Replay buffer
    parser.add_argument("--replay_buffer_size", type=int, default=10000, help="ç¼“å†²åŒºå¤§å°")
    # Model saving
    parser.add_argument("--save_model_freq", type=int, default=100, help="æ¨¡å‹ä¿å­˜é—´éš” episode")
    parser.add_argument("--model_save_path", type=str, default="./models/dqn_model.pth", help="æ¨¡å‹ä¿å­˜è·¯å¾„")
    parser.add_argument("--use_wandb", action="store_true", help="å¯ç”¨ wandb æ—¥å¿—è®°å½•ï¼ˆé»˜è®¤å…³é—­ï¼‰")
    parser.add_argument("--mix_mode", choices=["round", "episode"], default="round", help="å¯¹æ‰‹æ··åˆç²’åº¦")
    parser.add_argument("--heuristic_ratio", type=float, default=0.5, help="æ¯ä¸ªå¸­ä½ä½¿ç”¨å¯å‘å¼å¯¹æ‰‹çš„æ¦‚ç‡")
    parser.add_argument("--frozen_opponent_update_interval", type=int, default=100, help="æ›´æ–°å†»ç»“è‡ªåšå¼ˆå¯¹æ‰‹çš„ episode é—´éš”")
    parser.add_argument("--frozen_pool_size", type=int, default=5, help="ä¿å­˜å¤šå°‘ä¸ªå†»ç»“è‡ªåšå¼ˆå¯¹æ‰‹ç”¨äºè½®æ¢")
    parser.add_argument("--opponent_confidence", type=int, default=2, help="å¯å‘å¼å¯¹æ‰‹çš„æŒ‘æˆ˜æ¿€è¿›ç¨‹åº¦ï¼ˆç»ˆç‚¹ï¼‰")
    parser.add_argument("--opponent_conf_min", type=int, default=1, help="å¯å‘å¼å¯¹æ‰‹æŒ‘æˆ˜æ¿€è¿›åº¦èµ·ç‚¹")
    parser.add_argument("--curriculum_warmup", type=int, default=400, help="å¯¹æ‰‹æ¿€è¿›åº¦çº¿æ€§é€’è¿›æ‰€éœ€ episode æ•°")
    parser.add_argument("--frozen_interval_growth", type=float, default=2.0, help="å†»ç»“å¯¹æ‰‹åˆ·æ–°é—´éš”æ”¾å¤§å€æ•°")
    parser.add_argument("--eval_interval", type=int, default=100, help="è®­ç»ƒä¸­å¿«é€Ÿè¯„æµ‹çš„ episode é—´éš”")
    parser.add_argument("--eval_episodes", type=int, default=100, help="æ¯æ¬¡å¿«é€Ÿè¯„æµ‹çš„å¯¹å±€æ•°")
    parser.add_argument("--min_epsilon", type=float, default=0.1, help="æ¢ç´¢ç‡çš„æœ€å°å€¼")
    parser.add_argument("--epsilon_reset_interval", type=int, default=0, help="å‘¨æœŸæ€§é‡ç½® epsilon çš„ episode é—´éš”ï¼Œä¸º 0 åˆ™ä¸é‡ç½®")
    parser.add_argument("--epsilon_reset_value", type=float, default=0.2, help="é‡ç½®æ—¶çš„ epsilon å€¼")
    parser.add_argument("--n_step", type=int, default=3, help="n-step return çš„æ­¥é•¿")
    parser.add_argument("--shaping_beta", type=float, default=0.2, help="åŠ¿èƒ½å¥–åŠ± shaping ç³»æ•°ï¼ˆ<=0 å…³é—­ï¼‰")

    args = parser.parse_args()

    trained_agent = main(args)
